# WISDOM V2: Pairwise Comparisons

### Framework evolution
WISDOM V1 was based on the best known examples of open peer review in the open source (e.g., Mozilla) and open science (e.g., Open Science Leaders) spaces. Although this process works well for relatively monotonous offerings (e.g., applications to attend a hackathon), it cannot deal with highly heterogenous offerings across very different categories. For example, in the context of OHM Festival, we want to compare the relative value of very different contributions (e.g., cleaning toilets vs. playing a DJ set vs. administration tasks). This is difficult to achieve using a rubric, because we cannot capture the full range of value that people might add in a short text format. It would also place a burden of expertise on the reviewer (e.g., familiarity with the entire project), which limits participation from the broader crowd. 

This new version would aim to drastically simplify the problem space for the user, just asking them to compare two contributions on some dimension of interest. This is inspired by a common experimental paradigm, particularly in the psychological sciences, called a paired comparison task -- basically, the participant chooses which of two contributions (A or B) is more X (where X is some dimension of interest). Here, we would present users with two offerings/contributions and ask them to tell us which one they feel is more [valuable / worthy of gratitude / easy to complete / novel / etc.]. The actual questions we ask can be explored empirically, by collecting data and conducting metascience on the outcomes.

### Use cases
Tiny OHM #1 was a gift-based gathering held in Orleigh Park, West End, on 25th June 2022 from 10am-10pm. It was a public event attended by well over 100 people throughout the day and night. Prior to the event we invited the OHM community to apply to become one of the 'headline' acts at the gathering, resulting in 21 offerings. The OHM community were then invited to complete [reviews](https://docs.google.com/spreadsheets/d/1pQZDUxfWp-bcdKmXRrk9xqruLK9EkOj3TJ03FbdaIDM/edit?usp=sharing) of the offerings (WISDOM v2), resulting in a [ranked list](https://docs.google.com/spreadsheets/d/1UBonYNYcRWKW1PDkZH619Lo8JXC3QqYqPpKx5MDpAyI/edit?usp=sharing) of all 21 offerings generated by 21 reviewers. 5 offerings were selected as 'headline' acts and awarded priority timeslots, marketing support and a monetary bonus. All offerings were scheduled across two stages (music vs workshop) throughout the day. After the event we conducted another round of reviews, this time including a range of both scheduled and non-scheduled contributions (e.g., food and drink, production). This second dataset included the unique feature of our model, _meta-reviews_, and thus became the first proof-of-concept dataset for v2 of the complete WISDOM model. This second dataset is also much more interesting, so it is what the remainder of this report will focus on (see above links in this paragraph for the pre-review survey and results). 

## Methods
### Materials
- [Post-Tiny OHM Survey TEMPLATE (open access))](https://docs.google.com/spreadsheets/d/1tkwqzx2RmbYZXYHtkanUfepgWaoiAusp5NAXUO5jKNc/edit?usp=sharing)
- [Post-Tiny OHM Survey Backend (restricted)](https://docs.google.com/spreadsheets/d/1Z4Y0bLmKW8koYfsDj0iMaQP9yS4P6-WhScdvgzu4-wE/edit?usp=sharing)


### Participants
16 reviewers from the OHM community. 

### Process
These methods will be presented in chronological order using the OHM circular model stages of Record, Review, Recognise, Reward, and Respect 
~~~ 
NOTE THAT RESULTS ARE ALSO INCLUDED IN THIS SECTION AS PER THE AIMOS PRESENTATION, BUT COULD BE SEPARATED FOR CLARITY)
~~~

#### 1. Record
Tiny OHM contributions were recorded in the OHM Clickup workspace. From these records, the research team selected a diverse range of contributions including both contributions to the event and review contributions. 30 contributions were selected, including 27 event contributions (e.g., talks, music, donations) and 3 review contributions (completion of 1, 4 & 16 surveys, respectively). 

#### 2. Review
Reviewers vote between pairs of contributions on multiple dimensions of interest (see Materials above for survey links)

- 35 surveys (26 regular surveys with event contributions only, 9 ’expert’ surveys with event AND review contributions)
- Each survey contains 25 pairs (randomised)
- 4 dimensions per pair (2AFC):
_“Which contribution are you the most grateful for?”
“Which contribution was the most unique?”
“Which offering best supported the OHM vision and mission?”
“Which offering best supported our principle of Diversity and Inclusion?”_

#### 3. Recognise
Algorithms rank and benchmark contributions against pairwise comparisons. Meta-reviewers evaluate dimensions
1. Rank. Votes tallied for each contribution & dimension
2. Benchmark. Surveys awarded 1 OHMnom per pairwise comparison, fitted and used to scale rankings
- 1 survey (25 pairs) = 25 OHMnom
- 4 surveys (100 pairs) = 100 OHMnom
- 16 surveys (400 pairs) = 400 OHMnom
3. Meta-review. Validated experts review rankings and assign weights to each dimension.
4. Amalgamate. OHMnoms = average across meta-reviewer weightings

#### 4. Reward
Contributors are given OHMnoms as a token of gratitude. Excess tokens can be paid forward.
- OHMnoms given to each contributor (_Future: Divide collaborations using secondary review_)
- Subtract Tiny OHM ‘fee’ (average cost per ~111 participants = 77.68 OHMnom)
- Pay it forward use cases (say thanks to someone, gift to community, trade for project funding, etc.)

#### 5. Respect
Experts and reliable reviewers are identified and acknowledged with numerical indicators.
- Expertise = % of total OHMnoms in field
- Intra-rater reliability = % test-retest (within-subj)
- Inter-rater reliability = % test-retest (between-subj)
- Self-bias = % votes for own contribution / likelihood others vote for own contribution
- _Future: Reliable reviewers earn more OHMnoms for their reviews. Reputation scores as a function of token burns and expertise._

#### 6. Receive (included in slides, but new model will merge this with (4) and replace with "Replicate" to highlight the evolving nature instead)
Community members can meet their needs by application or by trading OHMnoms for resources.
- OHMniversal Basic Income scheme
  - 3 recipients prior to Tiny OHM (1 shown in results)
  - Contributions tracked and valued against grant
- Trading OHMnoms
  - Headline acts paid 0.802 OHMnom/$
  - Rent resources (e.g., speakers, OHMnibus, art)
  - _Future: radical market bidding_

### Results
- [Post-Tiny OHM Results](https://docs.google.com/spreadsheets/d/1HS7HFa9y6PfF61_wTuCAbj9RTHe94VLXulS0pDYgtns/edit?usp=sharing)
- [Video recording](https://youtu.be/NHgG599NoSk?si=CKAcxIM36oMvLlPc) and [slides](https://docs.google.com/presentation/d/1PDAqqHFCZsq3-z8pjbRob9QhcJ7vnTFwHxqXcuwGs2s/edit?usp=sharing) from our presentation at AIMOS 2023 conference
- TBC

### Learnings
- Pairwise comparisons are user-friendly
  - Surveys rated as moderately easy (28%) or very easy (48%); and comfortable (46%) or neutral (50%)
  - Average duration = 23 minutes (<1 min per comparison)
- Successes:
  - Autonomous, community-controlled system
  - Transparent, replicable, inclusive, diverse
  - First step toward an autonomous gathering template
- Limitations:
  - Did not identify all participants or contributions 
  - Skewed benchmarking for Uniqueness and Diversity
  - Could be improved with more meta-reviews (e.g., 2 & 8 surveys) or smarter ranking algorithm (e.g., ELO)

### Next steps
- Algorithm upgrades (ELO, priors)
  - Filter by explosure (cf. pairing algorithm)
  - Pre-vs post-event (‘super predictors’)
  - Reward regulation (e.g., with reliability)
- New dimensions (e.g., Necessity, Creativity, Difficulty, Generosity, Honesty, Clarity, Connection)
- AIMOS prototype


~~~
History: See original version in Clickup: https://app.clickup.com/t/2eb2xpn

