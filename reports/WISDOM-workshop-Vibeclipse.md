### Details
- When: 2pm Sun 7th April
- Where: Hearthtop @Vibeclipse
- Participants: 15+ attendees (names and handles written in Cooper's notebook but didn't get permission to share publicly)
- Host: Cooper
- Co-host: Angie

### Methods
#### Materials
- Love heart and star shapes cut out of coloured paper
- Coloured marker pens
- Chocolate easter eggs
- White and black string (not used)
- 15x straight sticks
- Notebook and pen

#### Preparation
- Create a circle with comfortable seats to sit (cushions or seats)
- Spread out 6 love hearts in the shape of a hexagram in centre of circle

#### Procedure
0. Introduction 
- Host invited a round to share names and reasons for joining workshop (as people filter in)
- Host thanked everyone for joining and gave a brief overview of WISDOM and workshop agenda
- Co-host took picture of everyone gathered

1. Nominating contributions
  - Host explained that first step in WISDOM protocol is to collect a list of contributions, gave some examples relevant to Vibeclipse
  - Co-host handed out love hearts 
  - Everyone wrote down one contribution including the contributor/s if known
- Sharing nominations
  - Each person shares their nomination with the group. Everyone else gives a '5-star rating' by holding up 0-5 fingers, host counts up the total number and writes on back of love heart. We did this a few times but was taking too long so 
  - After each person shares their nomination everyone else gives a 'rating' out of 5 by holding up a number of fingers and host tallies votes [was taking too long so after a couple times we switched to finger clicks as a way to show appreciation]
  - Co-host collects love hearts
 
2. Review
- Limitations of token-distribution / 5-star ratings (from literature)
    - Ceiling effects (e.g., Uber)
    - Drift effects
- Practical limitations (from OHM gatherings)
  - requires expertise to understand relative value of all contributions and rate fairly
  - requires lots of time to rate all contributions
  - complicated UX
- Pairwise comparisons as an inclusive review protocol
 - Cognitively simple
 - Quick and easy
 - Mobile-friendly UX
 - Produces rich dataset at the crowd-level
- Randomly select 5 love hearts, place in shape of a 'house' in the centre of the circle, use string to connect every possible pair [switched to using sticks because string was too wobbly]. 
- Crowd-voting
  - Explain that host will be playing the role of the pairing algorithm
  - Invite a volunteer to perform a review, pick up any two contributions and remove stick connecting them, ask volunteer which they are most grateful for, reward them with a chocolate easter egg, mark vote for (and against) on back of each love heart 
  - Keep going until all the sticks are removed
  - Host explained that we have now compared all contributions with one another and they have all had a fair chance to be voted for; this is what we did for Headliner review prior to [Tiny OHM](https://github.com/openheartmind/WISDOM/blob/main/reports/V2_pairwise-comparisons_Tiny-OHM-1.md), but cannot be scaled relative to other datasets

3. Meta-review
  - Introduce meta-review as novel feature of WISDOM, idea that trusted members of the community can rate reviews themselves, relative to event contributions
  - Host estimated a median value of pairwise comparisons (100 PCs) and wrote on star-shape cutout, invited volunteers to play role of meta-reviewer and compare this review contribution to each event contribution 
  - Placed all contributions in a row and explain that becase we know how many tokens the review contribution gets (100) we can use that to scale all others 

4. Economics
- Explain how information converts into OHMnom rewards for each contributor via transfer function, who can then pay forward to other people
- Explain how WISDOM can be energetically-neutral, by subtracting average OHMnoms from each contributor and leaving positive and negative balances for givers and receivers, respectively
- Invite questions

5. Governance
- Explain how we can develop expertise / reputation metrics by tracking the quality of contributions over time
- Explain how we can determine who are the reliable reviewers, thus earning them the role of Meta-reviewer and/or greater/lesser rewards for each review based on their reliability
- Invite questions

### Results

Reviewed contributions
| Contribution | Votes after review | Votes after meta-review |
| --- | --- | --- |
| Brooke for giving talk at NWS conference | X | Y |
| Scott for VC ops, finance etc. | X | Y |
| Parents for caring for children and inviting others along | X | Y |
| Quilted Rummage for their art installation | X | Y |
| Camp Champions staff for collecting firewood | X | Y |
| 100 pairwise comparisons (review contribution) | N/A | Z |

Non-reviewed contributions
| Contribution | 
| --- |
| Brundolf, site and warm/friendly/kind |
| Opening ceremony, staff |
| Vivek for priming on healthy request / rejection |
| Camp Champion staff (Ali) whi have contributed to vibe and been accommodating |
| Jane for hosting fun events |
| Mattias In Space for Mental Health Events |
| Agi, for leading the sparkle / glow social |
| DC for helping figure out where things are @Vibeclipse |
| The organizers for the tea house |
| Thanks to Katherine for thinking about the parents, big ups to her |
| Brooke - warm welcome |
| I am thankful to my mum for everything she's done for me and doing everything she can for me |
| I'd like to thank Dancing Horse for the dances |


### Improvements for next workshop
- after personal introductions: (a) ask permission to take photo AND share to social media, and (b) ask permission to record audio/video AND share to social media (with caveat anyone can revoke permission at any time prior to posting)
- upgrade coloured marker pens to all black
- highlight when giving out first easter egg (Stage 2) that incentives are the main obstacle to cultural change and adoption of new tech; WISDOM solves this by rewarding reviews directly (possible bc pairwise comparisons are relatively standard)
- might be better to add sticks until shape is complete, rather than remove them with each review
- after Stage 2 review could place all event contributions in a row to show interval results, while talking about Tiny OHM headliner review
- include FOUR regular contributions plus TWO meta-review contributions, so we can guard against high/low meta-review votes and demonstrate scaling (in this case we only got one vote for 100 PCs, placing it at the bottom of the pile, so could follow-up by voting for 200+ PCs)
