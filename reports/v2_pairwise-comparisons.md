# WISDOM V2: Pairwise Comparisons

### Overview


### Use cases
Tiny OHM #1 was a gift-based gathering held in Orleigh Park, West End, in June 2022. It was a public event attended by well over 100 people throughout the day and night. Prior to the event we invited the OHM community to apply to become one of the 'headline' acts at the gathering, resulting in 21 offerings. The OHM community were then invited to complete reviews of the offerings (WISDOM v2), resulting in a ranked list of all 21 offerings. 5 offerings were selected as 'headline' acts and given priority timeslots, marketing support and a monetary bonus. All other offerings were scheduled at other times throughout the day, across two stages (music vs workshop). After the event we conducted another round of reviews, this time including a range of both scheduled and non-scheduled contributions (e.g., food and drink, production). This second dataset included the unique feature of our model, _meta-reviews_, and thus became the first proof-of-concept dataset for our complete model.

### TBC...



~~~
Original version in Clickup: https://app.clickup.com/t/2eb2xpn

Definition
Third version of the WisdOHM of the Crowd protocol ( ), using a paired comparison task to determine the 'crowd-gratitude' and/or relative value of contributions to OHM.

Use cases
Prototype used to (a) select headline acts for  (post-event review was planned but never executed)

Details
 was based on the best known examples of open peer review in the open source (e.g., Mozilla) and open science (e.g., Open Science Leaders) spaces. Although this process works well for relatively monotonous offerings (e.g., applications to attend a hackathon or course), it cannot deal with highly heterogenous offerings across very different categories. For example, in the context of OHM Festival, we want to compare the relative value of wildly different contributions (e.g., cleaning the toilets vs. playing a DJ set vs. doing some administrative tasks). This is difficult to achieve using a rubric, because we cannot capture the full range of value that people might add in a short text format. It would also place a burden of expertise on the reviewer (e.g., familiarity with the entire project), which limits participation from the broader crowd. 

This new version would aim to drastically simplify the problem space for the user, just asking them to compare two contributions on some dimension of interest. This is inspired by a common experimental paradigm, particularly in the psychological sciences, called a paired comparison task -- basically, the participant chooses which of two contributions (A or B) is more X (where X is some dimension of interest). Here, we would present users with two offerings/contributions and ask them to tell us which one they feel is more [valuable / worthy of gratitude / easy to complete / novel / etc.]. The actual questions we ask can be explored empirically, by collecting data and conducting metascience on the outcomes.

Versions
WisdOHM of the crowd (v2.0.0) prototype
Credit & Gratitude
TBC
